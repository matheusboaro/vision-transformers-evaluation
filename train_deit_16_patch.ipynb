{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":["oDaKJgxzP9Fu"],"mount_file_id":"13mP5zy_6f7iQoz-_Fj5v_Tr5lE4TAQ2o","authorship_tag":"ABX9TyO31nA4tY5/CGNvKr4d149t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports and installations\n","- Pytorch\n","- Database import directly from Kaggle\n","\n","Data source: https://www.kaggle.com/xhlulu/140k-real-and-fake-faces"],"metadata":{"id":"i43SALPyP30W"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ti0cOXYIqglA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656072526899,"user_tz":180,"elapsed":5984,"user":{"displayName":"Sociedade do Café","userId":"06015360026721498871"}},"outputId":"bcb4f5b5-90bd-48eb-9b66-921169a40e73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.5.4)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n","mkdir: cannot create directory ‘/root/.kaggle}’: File exists\n"]}],"source":["!pip install timm\n","!pip install kaggle\n","! mkdir ~/.kaggle}\n","!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n","! chmod 600 ~/.kaggle/kaggle.json\n"]},{"cell_type":"code","source":["! kaggle datasets download xhlulu/140k-real-and-fake-faces\n","! unzip '140k-real-and-fake-faces.zip';"],"metadata":{"id":"S0BTWUFCqtPG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from json import load\n","import torch\n","import numpy as np\n","from torchvision.datasets import ImageFolder\n","from torchvision.transforms import ToTensor,Compose, Resize, Normalize\n","from torch.utils.data.dataloader import DataLoader\n","from torch.utils.data import random_split\n","#from vision_transformer import VisionTransformer\n","from torch import nn\n","import timm\n","import torch.nn.functional as F"],"metadata":{"id":"Z-t00ByAq54v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Early Stopping\n","Implements the early stopping technique based on the value of the loss function. Used to prevent the training set from overfitting.\n","\n","Source: https://github.com/Bjarten/early-stopping-pytorch\n"],"metadata":{"id":"oDaKJgxzP9Fu"}},{"cell_type":"code","source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"],"metadata":{"id":"7vKeMY0G3dxx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Auxiliary functions\n","- load_data : Loads the training, testing and validation set.\n","- acurracy : Calculates the accuracy of a set of logits.\n","- validation_on_end : Calculates the values of loss and acuracy per epoch and averages these values per batch.\n","- epoch_end: Prints the metrics during training, after each epoch.\n","- get_default_device: Identifies the GPU devices present in the environment.\n"],"metadata":{"id":"CoNT1B1dQYDk"}},{"cell_type":"code","source":["def load_data(path,train_rate,batch_size,test=False):\n","    NORMALIZE_MEAN = (0.5, 0.5, 0.5)\n","    NORMALIZE_STD = (0.5, 0.5, 0.5)\n","    transform = Compose([\n","              Resize(size=(224, 224)),\n","              ToTensor(),\n","              Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n","              ])\n","    \n","    dataset_train = ImageFolder(path+\"/train\", transform=transform)\n","    dataset_test = ImageFolder(path+\"/test\", transform=transform)\n","    dataset_valid =ImageFolder(path+\"/valid\", transform=transform)\n","\n","    train_dl = DataLoader(dataset_train,batch_size,shuffle=True)\n","    test_dl = DataLoader(dataset_test,batch_size)\n","    valid_dl = DataLoader(dataset_valid,batch_size)\n","    \n","    return train_dl,test_dl,valid_dl\n","\n","def accuracy(outputs, labels):\n","    _,preds = torch.max(outputs,dim = 1)\n","    return torch.tensor(torch.sum(preds==labels).item() / len(preds))\n","    \n","def validation_epoch_end(outputs):\n","        batch_loss = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_loss).mean()\n","        batch_acc = [x['val_acc'] for x in outputs]\n","        epoch_acc = torch.stack(batch_acc).mean()\n","\n","        return {\"val_loss\":epoch_loss.item(),'val_acc':epoch_acc.item()}\n","def epoch_end( epoch, result):\n","        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n","            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"],"metadata":{"id":"PUWM-88qrrNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_default_device():\n","\n","    if torch.cuda.is_available():\n","        return torch.device(\"cuda\")\n","    else:\n","        return torch.device(\"cpu\")\n","\n","def to_device(data,device):\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x,device) for x in data]\n","\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","\n","    def __init__(self,dl,device):\n","        self.dl = dl\n","        self.device = device\n","\n","    def __iter__(self):\n","        for b in self.dl:\n","            yield to_device(b,self.device)\n","\n","    def __len__(self):\n","        return len(self.dl)"],"metadata":{"id":"sc0mPfZWrzS3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training\n"],"metadata":{"id":"u3vOC4KaRdqh"}},{"cell_type":"markdown","source":["## Model Loading\n","  - DeiT patch 16\n","  \n","\n","\n","Source:\n","https://github.com/facebookresearch/dino"],"metadata":{"id":"ekM1ah8lRJXB"}},{"cell_type":"code","source":["\n","model= torch.hub.load('facebookresearch/deit:main', 'deit_base_distilled_patch16_224', pretrained=True)\n","\n","for params in model.parameters():\n","  params.requires_grad = False\n","\n","\n","model.head = nn.Linear(768,2)\n","model.head_dist = nn.Linear(768,2)\n","\n","model\n","\n"],"metadata":{"id":"RfuW8Y3xr1rD","executionInfo":{"status":"ok","timestamp":1656072699084,"user_tz":180,"elapsed":1528,"user":{"displayName":"Sociedade do Café","userId":"06015360026721498871"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e47ced67-3698-45a4-aecd-4c2cb0f49476"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_deit_main\n"]},{"output_type":"execute_result","data":{"text/plain":["DistilledVisionTransformer(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","    (norm): Identity()\n","  )\n","  (pos_drop): Dropout(p=0.0, inplace=False)\n","  (blocks): Sequential(\n","    (0): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","    (1): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","    (2): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","    (3): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","    (4): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","    (5): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","    (6): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","    (7): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","    (8): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","    (9): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","    (10): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","    (11): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU()\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","  )\n","  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","  (pre_logits): Identity()\n","  (head): Linear(in_features=768, out_features=2, bias=True)\n","  (head_dist): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## Training and Evaluation Functions\n","- evaluate: Evaluates the model with the validation set\n","- fit: trains the model for a specified number of epochs."],"metadata":{"id":"zRpjWw21Rakv"}},{"cell_type":"code","source":["@torch.no_grad()\n","def evaluate(model, val_loader):\n","    outputs =[]\n","    for batch in val_loader:\n","        out = model(batch[0])\n","        loss = F.cross_entropy(out[0],batch[1])\n","        acc = accuracy(out[0],batch[1])\n","        outputs.append({'val_loss': loss.detach(), \"val_acc\":acc})\n","    return validation_epoch_end(outputs)\n","        \n","\n","\n","def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD,path_to_save=''):\n","    history = []\n","    optimizer = opt_func(model.parameters(),lr)\n","    early_stopping = EarlyStopping(patience=4, verbose=True, path=path_to_save)\n","    for epoch in range(epochs):\n","        model.train()\n","        train_losses = []\n","        total = len(train_loader)\n","        for batch in train_loader:\n","            out = model(batch[0])\n","            loss_head = F.cross_entropy(out[0],batch[1])\n","            loss_head.backward()\n","            train_losses.append(loss_head)\n","            loss_dist = F.cross_entropy(out[1],batch[1])\n","            loss_dist.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        result = evaluate(model, val_loader)\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        epoch_end(epoch,result)\n","        early_stopping(result['val_loss'], model)\n","        \n","        if early_stopping.early_stop:\n","            print(\"Early stopping\")\n","            break\n","        history.append(result)\n","\n","    return history"],"metadata":{"id":"caCBHRiEryCg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main Script\n","- Database loading\n","- Transferring the model and the training and validation sets to the GPU\n","- Training the model"],"metadata":{"id":"2K_oSyiGSBZX"}},{"cell_type":"code","source":["train,test,valid = load_data('/content/real_vs_fake/real-vs-fake',0.8,256)\n","device = get_default_device()\n","\n","train = DeviceDataLoader(train,device)\n","valid = DeviceDataLoader(valid,device)\n","model.to(device);"],"metadata":{"id":"peXI4t5Xr-P0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 50\n","opt_func = torch.optim.Adam\n","lr = 0.001\n","path_to_save=''\n","history = fit(num_epochs, lr, model,train,valid, opt_func,path_to_save)"],"metadata":{"id":"mszw9MWEsL5N","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2930115c-c178-4ca7-9f95-b3c1f6356c5c","executionInfo":{"status":"ok","timestamp":1656093038444,"user_tz":180,"elapsed":4870184,"user":{"displayName":"Sociedade do Café","userId":"06015360026721498871"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [0], train_loss: 0.2079, val_loss: 0.2014, val_acc: 0.9219\n","Validation loss decreased (inf --> 0.201378).  Saving model ...\n","Epoch [1], train_loss: 0.1925, val_loss: 0.1909, val_acc: 0.9271\n","Validation loss decreased (0.201378 --> 0.190869).  Saving model ...\n","Epoch [2], train_loss: 0.1851, val_loss: 0.1851, val_acc: 0.9288\n","Validation loss decreased (0.190869 --> 0.185087).  Saving model ...\n","Epoch [3], train_loss: 0.1797, val_loss: 0.1824, val_acc: 0.9291\n","Validation loss decreased (0.185087 --> 0.182419).  Saving model ...\n","Epoch [4], train_loss: 0.1770, val_loss: 0.1795, val_acc: 0.9298\n","Validation loss decreased (0.182419 --> 0.179498).  Saving model ...\n","Epoch [5], train_loss: 0.1748, val_loss: 0.1770, val_acc: 0.9304\n","Validation loss decreased (0.179498 --> 0.176960).  Saving model ...\n","Epoch [6], train_loss: 0.1737, val_loss: 0.1776, val_acc: 0.9296\n","EarlyStopping counter: 1 out of 4\n","Epoch [7], train_loss: 0.1722, val_loss: 0.1759, val_acc: 0.9314\n","Validation loss decreased (0.176960 --> 0.175882).  Saving model ...\n","Epoch [8], train_loss: 0.1710, val_loss: 0.1743, val_acc: 0.9309\n","Validation loss decreased (0.175882 --> 0.174273).  Saving model ...\n","Epoch [9], train_loss: 0.1710, val_loss: 0.1786, val_acc: 0.9301\n","EarlyStopping counter: 1 out of 4\n","Epoch [10], train_loss: 0.1697, val_loss: 0.1726, val_acc: 0.9316\n","Validation loss decreased (0.174273 --> 0.172638).  Saving model ...\n","Epoch [11], train_loss: 0.1695, val_loss: 0.1744, val_acc: 0.9307\n","EarlyStopping counter: 1 out of 4\n","Epoch [12], train_loss: 0.1689, val_loss: 0.1732, val_acc: 0.9319\n","EarlyStopping counter: 2 out of 4\n","Epoch [13], train_loss: 0.1683, val_loss: 0.1726, val_acc: 0.9321\n","Validation loss decreased (0.172638 --> 0.172613).  Saving model ...\n","Epoch [14], train_loss: 0.1681, val_loss: 0.1733, val_acc: 0.9306\n","EarlyStopping counter: 1 out of 4\n","Epoch [15], train_loss: 0.1680, val_loss: 0.1710, val_acc: 0.9338\n","Validation loss decreased (0.172613 --> 0.171034).  Saving model ...\n","Epoch [16], train_loss: 0.1679, val_loss: 0.1712, val_acc: 0.9332\n","EarlyStopping counter: 1 out of 4\n","Epoch [17], train_loss: 0.1683, val_loss: 0.1718, val_acc: 0.9330\n","EarlyStopping counter: 2 out of 4\n","Epoch [18], train_loss: 0.1674, val_loss: 0.1710, val_acc: 0.9334\n","Validation loss decreased (0.171034 --> 0.170957).  Saving model ...\n","Epoch [19], train_loss: 0.1672, val_loss: 0.1722, val_acc: 0.9335\n","EarlyStopping counter: 1 out of 4\n","Epoch [20], train_loss: 0.1676, val_loss: 0.1729, val_acc: 0.9328\n","EarlyStopping counter: 2 out of 4\n","Epoch [21], train_loss: 0.1671, val_loss: 0.1719, val_acc: 0.9329\n","EarlyStopping counter: 3 out of 4\n","Epoch [22], train_loss: 0.1667, val_loss: 0.1703, val_acc: 0.9330\n","Validation loss decreased (0.170957 --> 0.170252).  Saving model ...\n","Epoch [23], train_loss: 0.1665, val_loss: 0.1706, val_acc: 0.9335\n","EarlyStopping counter: 1 out of 4\n","Epoch [24], train_loss: 0.1669, val_loss: 0.1707, val_acc: 0.9333\n","EarlyStopping counter: 2 out of 4\n","Epoch [25], train_loss: 0.1666, val_loss: 0.1709, val_acc: 0.9333\n","EarlyStopping counter: 3 out of 4\n","Epoch [26], train_loss: 0.1664, val_loss: 0.1728, val_acc: 0.9331\n","EarlyStopping counter: 4 out of 4\n","Early stopping\n"]}]}]}